{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparing the PCam data for training deep learning models using tensorflow dataset (tfds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that splits images and labels and one-hot-encodes the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample(sample):\n",
    "    image, label = sample['image'], sample['label']  \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.one_hot(label, 2, dtype=tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the tensorflow dataset API - tfds - to load the data from you local directory. First you have to download the data manually from https://syddanskuni-my.sharepoint.com/:f:/g/personal/cmd_sam_sdu_dk/EiWD2LmuxCJBp-_tfGK7aL8Bdt2cPsb6MCpVs1pOYUcXAw?email=cmd%40sam.sdu.dk&e=2Vx6tL and store it on you local drive. First some important remarks:\n",
    "- The folder structure is **crucial** and needs to be copied to your local drive. For example, if you with to use the folder denoted *c:\\user\\aml_take_home_exam* then you need to download the data to *c:\\user\\aml_take_home_exam\\patch_camelyon\\2.0.0*.\n",
    "- As you can see from the example below this implies that my data is stored locally under *V:\\BDADShareData2\\DATA_TFDS_BDAD\\patch_camelyon\\2.0.0*\n",
    "- By using tfds.load you do not need to load all data. If you only want 20% of the training data and 5% of test and validation data you simply specify **split=['train[:20%]','test[:5%]','validation[:5%]']**. In this case the variable ds1 contains only 20 percent of the training data while ds2 and ds3 contains 5% of the test and validation data respectively. This feature is important if you are facing hardware contraints.\n",
    "- You can read more about tensorflow dataset here https://www.tensorflow.org/datasets \n",
    "- **Note** If you wish not to use data stored locally you can use download=True and tensorflow will download the data for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1,ds2,ds3 = tfds.load('patch_camelyon:2.0.0',split=['train[:20%]','test[:5%]','validation[:5%]'],\n",
    "                        data_dir = r'C:\\Users\\maria\\OneDrive\\Skrivebord\\3. semester\\Anvendt maskinl√¶ring\\Exam',\n",
    "                        download=False,\n",
    "                        shuffle_files=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we simple transform the data (by the function convert sample described previously) and getting ready for training by splitting it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset       = ds1.map(convert_sample).batch(32)\n",
    "validation_dataset  = ds3.map(convert_sample).batch(32)\n",
    "test_dataset        = ds2.map(convert_sample).batch(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then ready to be applied for training, validation, testing etc...below just a very very simple illustration on how to construct and train a model based on the data we have prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1639/1639 [==============================] - 213s 130ms/step - loss: 0.6543 - accuracy: 0.5964 - val_loss: 0.6438 - val_accuracy: 0.6062\n",
      "Epoch 2/2\n",
      "1639/1639 [==============================] - 224s 137ms/step - loss: 0.5880 - accuracy: 0.6964 - val_loss: 0.6442 - val_accuracy: 0.6758\n"
     ]
    }
   ],
   "source": [
    "def first_ccn_model():\n",
    "    input_img = Input(shape=(96,96,3))\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='valid', activation='relu')(input_img)\n",
    "    x = Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    y = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=y)\n",
    "    return model\n",
    "\n",
    "sgd_opt = SGD(learning_rate=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "\n",
    "model = first_ccn_model()\n",
    "\n",
    "model.compile(optimizer=sgd_opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(train_dataset,\n",
    "                 validation_data=validation_dataset,\n",
    "                 epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hej "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlfall22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3cedec8935a2c28d6fd602c3007747750e2af1c4c937c29fac0d323bf1b544b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
