{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_random_sample(file_path, chunk_size, total_samples, random_state=42):\n",
    "    sample = pd.DataFrame()\n",
    "    total_rows = 0\n",
    "\n",
    "    try:\n",
    "        for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "            print(f\"Processing chunk: Total rows processed so far: {total_rows}\")\n",
    "            sample_chunk = chunk.sample(n=min(len(chunk), total_samples - total_rows), random_state=random_state)\n",
    "            sample = pd.concat([sample, sample_chunk], ignore_index=True)\n",
    "            total_rows += len(sample_chunk)\n",
    "            if total_rows >= total_samples:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(f\"Error occurred after processing {total_rows} rows.\")\n",
    "    \n",
    "    if sample.empty:\n",
    "        print(f\"No data loaded from file: {file_path}\")\n",
    "    return sample\n",
    "\n",
    "# Set the number of samples you want and the chunk size\n",
    "total_samples = 1000  # Number of reviews you want to sample\n",
    "chunk_size = 1000     # Size of each chunk to process, can be the same as total_samples\n",
    "\n",
    "\n",
    "# Paths to category files\n",
    "categories = {\n",
    "    #'Luxury_Beauty': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Luxury_Beauty.json',\n",
    "    'AMAZON_FASHION': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/AMAZON_FASHION.json',\n",
    "    'Clothing_Shoes_and_Jewelry': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Clothing_Shoes_and_Jewelry.json',\n",
    "    'All_Beauty': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/All_Beauty.json',\n",
    "    'Appliances': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Appliances.json',\n",
    "    'Toys_and_Games': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Toys_and_Games.json',\n",
    "    'Arts_Crafts_and_Sewing': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Arts_Crafts_and_Sewing.json',\n",
    "    'Grocery_and_Gourmet_Food': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Grocery_and_Gourmet_Food.json',\n",
    "    'Tools_and_Home_Improvement': '/Users/patrickbendorffschwebel/Desktop/Anvendt-kode/anvendt-exam/mappe uden navn 2/Tools_and_Home_Improvement.json'\n",
    "}\n",
    "\n",
    "\n",
    "# Load random samples for each category\n",
    "all_samples = pd.DataFrame()\n",
    "for category, path in categories.items():\n",
    "    df = load_random_sample(path, chunk_size, total_samples)\n",
    "    all_samples = pd.concat([all_samples, df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the data as needed\n",
    "def preprocess_text(text):\n",
    "    # Cleaning: Remove HTML tags and punctuation\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Normalization: Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Assume 'reviewText' is the column with the review texts and 'overall' is the ratings\n",
    "X = all_samples['reviewText']\n",
    "y = all_samples['overall']\n",
    "\n",
    "# Split the data equally into training and test sets (test_size=0.2- 80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define your pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classifier__class_weight': [None, 'balanced'],\n",
    "    'classifier__multi_class': ['ovr', 'multinomial'],\n",
    "    'classifier__max_iter': [100, 200, 500],\n",
    "    'classifier__tol': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Now that the model is fitted, we can access the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Use the full pipeline as the estimator\n",
    "estimator = grid_search.best_estimator_\n",
    "\n",
    "# Plot learning curve using the best estimator\n",
    "plot_learning_curve(best_model, 'Learning Curve', X_train, y_train, cv=5, n_jobs=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
